---
title: "Practical Machine Learning course"
author: "kuriboh"
date: "9/28/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract  
This report uses a dataset which generated by accelerometers in different body parts of participants who took part in an exercise.  
  
## Loads the libraries used in this report  
```{r, echo=FALSE}
library(caret)
```

## Loads the dataset into memory  
```{r}
train <- read.csv('data/pml-training.csv')
test <- read.csv('data/pml-testing.csv')
```  
  
## Perform some exploration on answering structure  
```{r}
'problem_id' %in% names(test)
'problem_id' %in% names(train)

'classe' %in% names(test)
'classe' %in% names(train)
unique(train$classe)
```
## Data cleaning  
After performing some other exploratory analysis on the dataset, I decided to use only the sensor measurements for prediction.  
* Removes useless variables  
```{r}
variables_to_be_removed <- c(
    'X',
    'user_name',
    'raw_timestamp_part_1',
    'raw_timestamp_part_2',
    'cvtd_timestamp',
    'new_window',
    'num_window'
)
head(train)
remove_columns_by_name <- function (dataset, columns) {
    for (variable in columns) {
        dataset[variable] <- NULL
    }
    dataset
}
train <- remove_columns_by_name(train, variables_to_be_removed)
test <- remove_columns_by_name(test, variables_to_be_removed)
```
  
* Gets the problem_id(s) from the test dataset and classe from the train datasets  
```{r}
ids <- as.data.frame(test$problem_id)
labels <- as.data.frame(as.factor(train$classe))
train$classe <- NULL
test$problem_id <- NULL
```
  
* Converts all values to numeric* Convert values in the dataframe into numeric ones  
```{r}
convert_to_numeric <- function(x) {
    as.numeric(as.character(as.numeric(x)))
}
numeric_factor <- function(dataset) {
    modifyList(dataset,
               lapply(dataset[, sapply(dataset, is.factor)], convert_to_numeric))
}
train <- numeric_factor(train)
test <- numeric_factor(test)
```
  
* Copes with the missing values, center and scale  
```{r}
preprocessed_train <- preProcess(train, method = c("medianImpute", "center", "scale"))
train <- predict(preprocessed_train, train)
test <- predict(preprocessed_train, test)
```
  
* Dimension reduction using PCA  
```{r}
reduced_data <- preProcess(train, method="pca", thresh=0.95)
train <- predict(reduced_data, train)
test <- predict(reduced_data, test)
```
  
* Re-attaches labels to the train data  
```{r}
names(labels) <- 'labels'
train <- cbind(train, labels)
```
  
* Cross validation  
For this step, I split the training dataset into two parts with ratio of 8:2.  
```{r}
splited <- createDataPartition(y=train$labels, p=0.8, list=FALSE)
cross_train <- train[splited, ]
cross_test <- train[-splited, ]
```
  
* Trains the model with gradient boosting machine  
```{r, echo=FALSE}
fit_model <- train(as.factor(cross_train$labels) ~ ., method="gbm", data=cross_train)
```
  
* Generate the prediction with the trained model  
```{r}
prediction <- predict(fit_model, cross_test[, -1])
```
  
* Calculate the accuracy  
```{r}
accuracy <- sum((prediction == cross_test$label)) / length(cross_test$label)
print(paste("Accuracy is:", accuracy))
```

* Just for easier visualization, I parse the output into a csv file  
```{r}
result <- fit_model(fit, test)
result <- as.data.frame(result)
output <- data.frame(problem_id = ids, label = result)
write.csv(output, file = paste("result/output.csv", sep=""), row.names = FALSE)
```